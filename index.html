<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenAI Real-time Audio Demo</title>
    <link rel="stylesheet" href="/static/styles.css">
</head>
<body>
    <div class="container">
        <h1>Friendly Êó•Êú¨Ë™û (Japanese) Chatbot</h1>
        
        <div class="controls" style="text-align: center; margin-bottom: 2rem;">
            <button id="startButton">Start</button>
            <div id="status" class="status">Ready to start</div>
            <button id="stopButton" disabled>Stop</button>
        </div>

        <div class="transcript-container">
            <div id="transcript" class="transcript"></div>
        </div>

        <div class="bottom-controls">
            <button id="saveButton" class="secondary-button" disabled>Save Conversation</button>
            <button id="viewSavedButton" class="secondary-button">View Saved</button>
            <select id="languageSelect" class="language-select">
                <option value="ja-JP">Japanese</option>
                <option value="">Auto Detect</option>
                <option value="en-US">English</option>
                <option value="es-ES">Spanish</option>
                <option value="fr-FR">French</option>
                <option value="de-DE">German</option>
                <option value="ko-KR">Korean</option>
                <option value="zh-CN">Chinese (Simplified)</option>
            </select>
        </div>

        <div id="error" class="error"></div>
    </div>

    <!-- Add this modal structure -->
    <div id="savedConversationsModal" class="modal">
        <div class="modal-content">
            <span class="close">&times;</span>
            <h2>Saved Conversations</h2>
            <div id="savedConversationsList"></div>
        </div>
    </div>

    <script>
        // UI elements
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        const transcriptDiv = document.getElementById('transcript');
        const statusDiv = document.getElementById('status');
        const errorDiv = document.getElementById('error');
        const saveButton = document.getElementById('saveButton');
        const viewSavedButton = document.getElementById('viewSavedButton');
        const modal = document.getElementById('savedConversationsModal');
        const closeBtn = document.querySelector('.close');
        const savedConversationsList = document.getElementById('savedConversationsList');
        const languageSelect = document.getElementById('languageSelect');

        let peerConnection = null;
        let audioStream = null;
        let dataChannel = null;

        let currentConversation = {
            messages: [],
            timestamp: null
        };

        // --- NEW LOGIC FOR ORDERING ---
        // userCount is how many user transcripts are "available" to pair with an AI response
        let userCount = 0;
        // responseQueue holds AI responses that arrived before any user transcript
        const responseQueue = [];

        // Display user transcript in the UI
        function handleUserTranscript(transcribedText) {
            const p = document.createElement('p');
            p.innerHTML = `<strong>USER:</strong> ${transcribedText}`;
            transcriptDiv.appendChild(p);

            currentConversation.messages.push({
                role: 'user',
                text: transcribedText
            });

            // Increase userCount because we have a user transcript available
            userCount++;

            // If there are any AI responses waiting, display them if userCount > 0
            while (responseQueue.length > 0 && userCount > 0) {
                const nextAI = responseQueue.shift();
                displayAITranscript(nextAI);
                userCount--;
            }
        }

        // Actually show the AI transcript in the UI
        function displayAITranscript(message) {
            if (!message.response?.output?.[0]?.content?.[0]?.transcript) return;
            const text = message.response.output[0].content[0].transcript;
            const p = document.createElement('p');
            p.setAttribute('data-ai', 'true');
            p.innerHTML = `<strong>AI:</strong> `;
            transcriptDiv.appendChild(p);

            currentConversation.messages.push({
                role: 'ai',
                text: text
            });

            // Animate text character by character
            let index = 0;
            const interval = setInterval(() => {
                if (index < text.length) {
                    p.innerHTML = `<strong>AI:</strong> ${text.substring(0, index + 1)}`;
                    index++;
                } else {
                    clearInterval(interval);
                }
            }, 30);
        }

        // Animate AI messages (decides if we can show them right away or queue them)
        function handleTranscript(message) {
            // If userCount is at least 1, we can display this AI response immediately
            if (userCount > 0) {
                displayAITranscript(message);
                userCount--; 
            } else {
                // Otherwise, queue it until we receive a user transcript
                responseQueue.push(message);
            }
        }

        // Handle the weather function
        async function handleWeatherFunction(output) {
            try {
                const args = JSON.parse(output.arguments);
                const location = args.location;
                
                const response = await fetch(`http://localhost:8888/weather/${encodeURIComponent(location)}`);
                const data = await response.json();
                
                // Send function output
                sendFunctionOutput(output.call_id, {
                    temperature: data.temperature,
                    unit: data.unit,
                    location: location
                });
                
                // Request new response
                sendResponseCreate();
            } catch (error) {
                showError('Error handling weather function: ' + error.message);
            }
        }

        // Check if there's a function call and handle it
        function handleFunctionCall(output) {
            if (output?.type === "function_call" && 
                output?.name === "get_weather" && 
                output?.call_id) {
                console.log('Function call found:', output);
                handleWeatherFunction(output);
            }
        }

        // Process incoming messages
        function handleMessage(event) {
            try {
                const message = JSON.parse(event.data);
                console.log('Received message:', message);

                switch (message.type) {
                    // This event is triggered when user audio is transcribed on the server
                    case "conversation.item.input_audio_transcription.completed":
                        handleUserTranscript(message.transcript);
                        break;

                    case "response.done":
                        handleTranscript(message);
                        const output = message.response?.output?.[0];
                        if (output) handleFunctionCall(output);
                        break;

                    default:
                        console.log('Unhandled message type:', message.type);
                }
            } catch (error) {
                showError('Error processing message: ' + error.message);
            }
        }

        // WebRTC audio setup
        async function setupAudio() {
            const audioEl = document.createElement("audio");
            audioEl.autoplay = true;
            
            peerConnection.ontrack = e => audioEl.srcObject = e.streams[0];
            
            audioStream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                }
            });
            
            peerConnection.addTrack(audioStream.getTracks()[0]);
        }

        // Data channel setup
        function setupDataChannel() {
            dataChannel = peerConnection.createDataChannel("oai-events");
            dataChannel.onopen = onDataChannelOpen;
            dataChannel.addEventListener("message", handleMessage);
        }

        // Session update to configure transcription and other settings
        function sendSessionUpdate() {
            const sessionUpdateEvent = {
                type: "session.update",
                session: {
                    voice: "alloy",
                    instructions: "Call provided tools if appropriate for the user's input.",
                    input_audio_format: "pcm16",
                    input_audio_transcription: {
                        model: "whisper-1"
                    },
                    turn_detection: {
                        threshold: 0.4,
                        silence_duration_ms: 600,
                        type: "server_vad"
                    },
                    tools: [{
                        type: "function",
                        name: "get_weather",
                        description: "Get the current weather. Works only for Earth",
                        parameters: {
                            type: "object",
                            properties: {
                                location: { type: "string" }
                            },
                            required: ["location"]
                        }
                    }],
                    tool_choice: "auto"
                }
            };
            sendMessage(sessionUpdateEvent);
        }

        // Send initial user message to define chatbot behavior
        function sendInitialMessage() {
            const conversationMessage = {
                "type": "conversation.item.create",
                "previous_item_id": null,
                "item": {
                    "id": "msg_" + Date.now(),
                    "type": "message",
                    "role": "user",
                    "content": [{
                        "type": "input_text",
                        "text": "Êó•Êú¨Ë™û„Åß„ÇÜ„Å£„Åè„Çä„ÄÅ„Åù„Åó„Å¶„Éï„É¨„É≥„Éâ„É™„Éº„Å™„Éà„Éº„É≥„ÅßË©±„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åô„Åπ„Å¶„ÅÆËøîÁ≠î„Çí„Å®„Å¶„ÇÇÁü≠„Åè„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
                    }]
                }
            };
            sendMessage(conversationMessage);
        }

        // Send function output
        function sendFunctionOutput(callId, data) {
            const responseMessage = {
                "type": "conversation.item.create",
                "item": {
                    "type": "function_call_output",
                    "call_id": callId,
                    "output": JSON.stringify(data)
                }
            };
            sendMessage(responseMessage);
        }

        // Request the AI model to generate a new response
        function sendResponseCreate() {
            sendMessage({ "type": "response.create" });
        }

        // Generic send message utility
        function sendMessage(message) {
            if (dataChannel?.readyState === "open") {
                dataChannel.send(JSON.stringify(message));
                console.log('Sent message:', message);
            }
        }

        // Called when data channel is open
        function onDataChannelOpen() {
            // Configure session for server-side transcription, etc.
            sendSessionUpdate();
            // Send an initial user message
            sendInitialMessage();
        }

        // Initialize connection
        async function init() {
            startButton.disabled = true;
            
            try {
                updateStatus('Initializing...');

                // The normal WebRTC flow
                const tokenResponse = await fetch("http://localhost:8888/session");
                const data = await tokenResponse.json();
                const EPHEMERAL_KEY = data.client_secret.value;

                peerConnection = new RTCPeerConnection();
                await setupAudio();
                setupDataChannel();

                const offer = await peerConnection.createOffer();
                await peerConnection.setLocalDescription(offer);

                const baseUrl = "https://api.openai.com/v1/realtime";
                const model = "gpt-4o-realtime-preview-2024-12-17";
                const sdpResponse = await fetch(`${baseUrl}?model=${model}`, {
                    method: "POST",
                    body: offer.sdp,
                    headers: {
                        Authorization: `Bearer ${EPHEMERAL_KEY}`,
                        "Content-Type": "application/sdp"
                    },
                });

                const answer = {
                    type: "answer",
                    sdp: await sdpResponse.text(),
                };
                await peerConnection.setRemoteDescription(answer);

                updateStatus('Connected');
                stopButton.disabled = false;
                hideError();

            } catch (error) {
                startButton.disabled = false;
                stopButton.disabled = true;
                showError('Error: ' + error.message);
                console.error('Initialization error:', error);
                updateStatus('Failed to connect');
            }

            saveButton.disabled = false;
        }

        // Stop everything
        function stopRecording() {
            if (peerConnection) {
                peerConnection.close();
                peerConnection = null;
            }
            if (audioStream) {
                audioStream.getTracks().forEach(track => track.stop());
                audioStream = null;
            }
            if (dataChannel) {
                dataChannel.close();
                dataChannel = null;
            }

            startButton.disabled = false;
            stopButton.disabled = true;
            updateStatus('Ready to start');
        }

        // UI helpers
        function updateStatus(message) {
            statusDiv.textContent = message;
        }
        function showError(message) {
            errorDiv.style.display = 'block';
            errorDiv.textContent = message;
        }
        function hideError() {
            errorDiv.style.display = 'none';
        }

        // Event listeners
        startButton.addEventListener('click', init);
        stopButton.addEventListener('click', stopRecording);

        document.addEventListener('DOMContentLoaded', () => updateStatus('Ready to start'));

        saveButton.addEventListener('click', saveCurrentConversation);
        viewSavedButton.addEventListener('click', () => {
            loadSavedConversations();
            modal.style.display = 'block';
        });
        closeBtn.addEventListener('click', () => {
            modal.style.display = 'none';
        });
        window.addEventListener('click', (event) => {
            if (event.target === modal) {
                modal.style.display = 'none';
            }
        });

        // When language is changed, we do nothing special now since local speech is removed
        languageSelect.addEventListener('change', () => {
            // No-op for server transcription
        });

        // Conversation management
        function saveCurrentConversation() {
            if (currentConversation.messages.length === 0) return;
            
            const conversations = JSON.parse(localStorage.getItem('conversations') || '[]');
            currentConversation.timestamp = new Date().toISOString();
            conversations.push(currentConversation);
            localStorage.setItem('conversations', JSON.stringify(conversations));
            
            showError('Conversation saved successfully!');
            setTimeout(hideError, 2000);
        }

        function deleteConversation(index, event) {
            event.stopPropagation(); 
            if (confirm('Are you sure you want to delete this conversation?')) {
                const conversations = JSON.parse(localStorage.getItem('conversations') || '[]');
                conversations.splice(conversations.length - 1 - index, 1);
                localStorage.setItem('conversations', JSON.stringify(conversations));
                loadSavedConversations();
            }
        }

        function loadSavedConversations() {
            const conversations = JSON.parse(localStorage.getItem('conversations') || '[]');
            savedConversationsList.innerHTML = '';
            
            conversations.reverse().forEach((conv, index) => {
                const div = document.createElement('div');
                div.className = 'saved-conversation-item';
                const date = new Date(conv.timestamp).toLocaleString();
                const preview = conv.messages[0]?.text || 'Empty conversation';
                
                div.innerHTML = 
                    `<div class="conversation-content">
                        <strong>${date}</strong>
                        <div class="conversation-preview">${preview.substring(0, 100)}...</div>
                    </div>
                    <button class="delete-conversation" title="Delete conversation">üóëÔ∏è</button>`;
                
                // Add click handler for loading conversation
                div.querySelector('.conversation-content').onclick = () => loadConversation(conversations.length - 1 - index);
                
                // Add click handler for delete button
                div.querySelector('.delete-conversation').onclick = (e) => deleteConversation(index, e);
                
                savedConversationsList.appendChild(div);
            });
        }

        function loadConversation(index) {
            const conversations = JSON.parse(localStorage.getItem('conversations') || '[]');
            const conversation = conversations[index];
            
            transcriptDiv.innerHTML = '';
            conversation.messages.forEach(msg => {
                const p = document.createElement('p');
                if (msg.role === 'ai') {
                    p.setAttribute('data-ai', 'true');
                }
                p.innerHTML = `<strong>${msg.role.toUpperCase()}:</strong> ${msg.text}`;
                transcriptDiv.appendChild(p);
            });
            
            currentConversation = {
                messages: [...conversation.messages],
                timestamp: conversation.timestamp
            };
            
            modal.style.display = 'none';
        }
    </script>
</body>
</html>
